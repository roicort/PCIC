{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e493759d",
   "metadata": {},
   "source": [
    "# Procesamiento del Lenguaje Natural\n",
    "\n",
    "Rodrigo S. Cortez Madrigal\n",
    "\n",
    "<img src=\"https://pcic.posgrado.unam.mx/wp-content/uploads/Ciencia-e-Ingenieria-de-la-Computacion_color.png\" alt=\"Logo PCIC\" width=\"128\" />  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "442e0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import glob\n",
    "import os\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.vq import whiten\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b82aea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['corpus/casatomada.txt', 'corpus/elevangeliosegunmarcos.txt', 'corpus/elaleph.txt', 'corpus/cartaaunasenoritaenparis.txt']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "corpus_path = \"corpus\"  # Ruta de la carpeta donde están los archivos .txt\n",
    "files = glob.glob(os.path.join(corpus_path, \"*.txt\"))  # Buscar todos los .txt en la carpeta\n",
    "textos = []\n",
    "for fn in files:\n",
    "    with open(fn, encoding=\"utf-8\") as f:  # Asegurarse de usar la codificación adecuada\n",
    "        textos.append(f.read())\n",
    "all_text = ' '.join(textos)\n",
    "print(files)\n",
    "\n",
    "def PredictAuthors(fvs):\n",
    "    '''\n",
    "    Se utiliza el algoritmo de K-Medias como método de clusterización (No supervisado)\n",
    "    '''\n",
    "    km = KMeans(n_clusters=2, init='k-means++', n_init=20, verbose=0)\n",
    "    km.fit(fvs)\n",
    "    return km\n",
    "def Features(textos):\n",
    "    '''\n",
    "    Obtener vectores de características, utilizando características estilométricas de caracteres, léxicas y sintácticas.\n",
    "    '''\n",
    "    num_textos = len(textos)\n",
    "    fvs = np.zeros((len(textos), 9), np.float64)\n",
    "    for e, ch_text in enumerate(textos):\n",
    "        contMayus = 0\n",
    "        contMinus = 0\n",
    "        contNumeros = 0\n",
    "        textoNormal = ch_text\n",
    "        textoMinusculas = ch_text.lower()\n",
    "        charTotales = len(textoNormal)\n",
    "        texto = ch_text.lower()\n",
    "        tokens = nltk.word_tokenize(texto)\n",
    "        words = word_tokenizer.tokenize(texto)\n",
    "        sentences = sentence_tokenizer.tokenize(ch_text)\n",
    "        vocab = set(words)\n",
    "        words_per_sentence = np.array([len(word_tokenizer.tokenize(s))\n",
    "                                       for s in sentences])\n",
    "        for letra in textoNormal:\n",
    "            if letra.isupper():\n",
    "                contMayus = contMayus + 1\n",
    "            else:\n",
    "                contMinus = contMinus + 1\n",
    "            if letra in [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"0\"]:\n",
    "                contNumeros = contNumeros + 1\n",
    "        # Número promedio de palabras por oración\n",
    "        fvs[e, 0] = words_per_sentence.mean()\n",
    "        # Variación del tamaño de las oraciones\n",
    "        fvs[e, 1] = words_per_sentence.std()\n",
    "        # Diversidad léxica\n",
    "        fvs[e, 2] = len(vocab) / float(len(words))\n",
    "        # Número de comas por oración\n",
    "        fvs[e, 3] = tokens.count(',') / float(len(sentences))\n",
    "        # Número de puntos y comas por oración\n",
    "        fvs[e, 4] = tokens.count(';') / float(len(sentences))\n",
    "        # Número de dos puntos por oración\n",
    "        fvs[e, 5] = tokens.count(':') / float(len(sentences))\n",
    "        # Proporción mayúsculas/total de caracteres\n",
    "        fvs[e, 6] = float(contMayus/charTotales)\n",
    "\t\t# Proporción minúsculas/total de caracteres\n",
    "        fvs[e, 7] = float(contMinus/charTotales)\n",
    "        # Proporción de números respecto a letras\n",
    "        fvs[e, 8] = float(contNumeros/charTotales)\n",
    "\n",
    "    fvs = whiten(fvs)\n",
    "    return fvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b86f8deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "feature_sets = []\n",
    "for element in list(Features(textos)):\n",
    "    feature_sets.append(element)\n",
    "feature_sets = np.array(feature_sets)\n",
    "model = PredictAuthors(feature_sets)\n",
    "result = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75d03781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aed434",
   "metadata": {},
   "source": [
    "# Otra forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "01178041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "def get_Style(textos):\n",
    "    lem_textos = []\n",
    "    nlp = spacy.load(\"es_core_news_sm\")\n",
    "    for ch_text in textos:\n",
    "        doc = nlp(ch_text)\n",
    "        lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        lem_textos.append(' '.join(lemmas))\n",
    "\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(lem_textos)\n",
    "    return X.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dc809cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = []\n",
    "feature_sets = []\n",
    "for element in list(get_Style(textos)):\n",
    "    feature_sets.append(element)\n",
    "model = PredictAuthors(feature_sets)\n",
    "result = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6dca58bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
