
@article{searle_minds_1980,
	title = {Minds, brains, and programs},
	volume = {3},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X00005756/type/journal_article},
	doi = {10.1017/S0140525X00005756},
	abstract = {This article can be viewed as an attempt to explore the consequences of two propositions. (1) Intentionality in human beings (and animals) is a product of causal features of the brain. I assume this is an empirical fact about the actual causal relations between mental processes and brains. It says simply that certain brain processes are sufficient for intentionality. (2) Instantiating a computer program is never by itself a sufficient condition of intentionality. The main argument of this paper is directed at establishing this claim. The form of the argument is to show how a human agent could instantiate the program and still not have the relevant intentionality. These two propositions have the following consequences: (3) The explanation of how the brain produces intentionality cannot be that it does it by instantiating a computer program. This is a strict logical consequence of 1 and 2. (4) Any mechanism capable of producing intentionality must have causal powers equal to those of the brain. This is meant to be a trivial consequence of 1. (5) Any attempt literally to create intentionality artificially (strong AI) could not succeed just by designing programs but would have to duplicate the causal powers of the human brain. This follows from 2 and 4. "Could a machine think?" On the argument advanced here only a machine could think, and only very special kinds of machines, namely brains and machines with internal causal powers equivalent to those of brains. And that is why strong AI has little to tell us about thinking, since it is not about machines but about programs, and no program by itself is sufficient for thinking.},
	language = {en},
	number = {3},
	urldate = {2024-08-20},
	journal = {Behavioral and Brain Sciences},
	author = {Searle, John R.},
	month = sep,
	year = {1980},
	pages = {417--424},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/U87DAXWV/Searle - 1980 - Minds, brains, and programs.pdf:application/pdf},
}

@article{cortes_racionalidad_nodate,
	title = {Racionalidad {Computacional}},
	language = {es},
	author = {Cortés, Luis Alberto Pineda},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/LAUSUNYK/Cortés - Racionalidad Computacional.pdf:application/pdf},
}

@article{pineda_mode_2024,
	title = {The mode of computing},
	volume = {84},
	issn = {13890417},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1389041723001389},
	doi = {10.1016/j.cogsys.2023.101204},
	abstract = {The Turing Machine is the paradigmatic case of computing machines, but there are others such as analogical, connectionist, quantum and diverse forms of unconventional computing, each based on a particular intuition of the phenomenon of computing. This variety can be captured in terms of system levels, re-interpreting and generalizing Newell’s hierarchy, which includes the knowledge level at the top and the symbol level immediately below it. In this re-interpretation the knowledge level consists of human knowledge and the symbol level is generalized into a new level that here is called The Mode of Computing. Mental processes performed by natural brains are often thought of informally as computing processes and that the brain is alike to computing machinery. However, if natural computing does exist it should be characterized on its own. A proposal to such an effect is that natural computing appeared when interpretations were first made by biological entities, so natural computing and interpreting are two aspects of the same phenomenon, or that consciousness and experience are the manifestations of computing/interpreting. By analogy with computing machinery, there must be a system level at the top of the neural circuitry and directly below the knowledge level that is named here The mode of Natural Computing. If it turns out that such putative object does not exist the proposition that the mind is a computing process should be dropped; but characterizing it would come with solving the hard problem of consciousness.},
	language = {en},
	urldate = {2024-10-15},
	journal = {Cognitive Systems Research},
	author = {Pineda, Luis A.},
	month = mar,
	year = {2024},
	pages = {101204},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/EEV4AUEQ/Pineda - 2024 - The mode of computing.pdf:application/pdf},
}

@article{piaget_jean_1981,
	title = {Jean {Piaget}: {Six} {Psychological} {Studies}. {Edited} by {D}. {Elkind}.},
	volume = {11},
	copyright = {https://www.cambridge.org/core/terms},
	issn = {0033-2917, 1469-8978},
	shorttitle = {Jean {Piaget}},
	url = {https://www.cambridge.org/core/product/identifier/S0033291700053757/type/journal_article},
	doi = {10.1017/S0033291700053757},
	language = {en},
	number = {1},
	urldate = {2024-10-15},
	journal = {Psychological Medicine},
	author = {Piaget, Jean},
	month = feb,
	year = {1981},
	pages = {207--207},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/7ZI97XMZ/1981 - Jean Piaget Six Psychological Studies. Edited by D. Elkind. (Pp. 169\; illustrated\; £13.50 hb, £4.95.pdf:application/pdf},
}

@article{posner_research_2007,
	title = {Research on {Attention} {Networks} as a {Model} for the {Integration} of {Psychological} {Science}},
	volume = {58},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.psych.58.110405.085516},
	doi = {10.1146/annurev.psych.58.110405.085516},
	abstract = {As Titchener pointed out more than one hundred years ago, attention is at the center of the psychological enterprise. Attention research investigates how voluntary control and subjective experience arise from and regulate our behavior. In recent years, attention has been one of the fastest growing of all ﬁelds within cognitive psychology and cognitive neuroscience. This review examines attention as characterized by linking common neural networks with individual differences in their efﬁcient utilization. The development of attentional networks is partly speciﬁed by genes, but is also open to speciﬁc experiences through the actions of caregivers and the culture. We believe that the connection between neural networks, genes, and socialization provides a common approach to all aspects of human cognition and emotion. Pursuit of this approach can provide a basis for psychology that uniﬁes social, cultural, differential, experimental, and physiological areas, and allows normal development to serve as a baseline for understanding various forms of pathology. D.O. Hebb proposed this approach 50 years ago in his volume Organization of Behavior and continued with introductory textbooks that dealt with all of the topics of psychology in a common framework. Use of a common network approach to psychological science may allow a foundation for predicting and understanding human behavior in its varied forms.},
	language = {en},
	number = {1},
	urldate = {2024-10-15},
	journal = {Annual Review of Psychology},
	author = {Posner, Michael I. and Rothbart, Mary K.},
	month = jan,
	year = {2007},
	pages = {1--23},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/JF2CRB9V/Posner y Rothbart - 2007 - Research on Attention Networks as a Model for the Integration of Psychological Science.pdf:application/pdf},
}

@article{tulving_episodic_2002,
	title = {Episodic {Memory}: {From} {Mind} to {Brain}},
	volume = {53},
	issn = {0066-4308, 1545-2085},
	shorttitle = {Episodic {Memory}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev.psych.53.100901.135114},
	doi = {10.1146/annurev.psych.53.100901.135114},
	abstract = {[Figure: see text]
            ▪ Abstract  Episodic memory is a neurocognitive (brain/mind) system, uniquely different from other memory systems, that enables human beings to remember past experiences. The notion of episodic memory was first proposed some 30 years ago. At that time it was defined in terms of materials and tasks. It was subsequently refined and elaborated in terms of ideas such as self, subjective time, and autonoetic consciousness. This chapter provides a brief history of the concept of episodic memory, describes how it has changed (indeed greatly changed) since its inception, considers criticisms of it, and then discusses supporting evidence provided by (a) neuropsychological studies of patterns of memory impairment caused by brain damage, and (b) functional neuroimaging studies of patterns of brain activity of normal subjects engaged in various memory tasks. I also suggest that episodic memory is a true, even if as yet generally unappreciated, marvel of nature.},
	language = {en},
	number = {1},
	urldate = {2024-10-15},
	journal = {Annual Review of Psychology},
	author = {Tulving, Endel},
	month = feb,
	year = {2002},
	pages = {1--25},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/Q3HZ244Y/Tulving - 2002 - Episodic Memory From Mind to Brain.pdf:application/pdf},
}

@article{hassabis_deconstructing_2007,
	title = {Deconstructing episodic memory with construction},
	volume = {11},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661307001258},
	doi = {10.1016/j.tics.2007.05.001},
	language = {en},
	number = {7},
	urldate = {2024-10-15},
	journal = {Trends in Cognitive Sciences},
	author = {Hassabis, Demis and Maguire, Eleanor A.},
	month = jul,
	year = {2007},
	pages = {299--306},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/AZEDV3LH/Hassabis y Maguire - 2007 - Deconstructing episodic memory with construction.pdf:application/pdf},
}

@article{hassabis_using_2007,
	title = {Using {Imagination} to {Understand} the {Neural} {Basis} of {Episodic} {Memory}},
	volume = {27},
	copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
	issn = {0270-6474, 1529-2401},
	url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.4549-07.2007},
	doi = {10.1523/JNEUROSCI.4549-07.2007},
	abstract = {Functional MRI (fMRI) studies investigating the neural basis of episodic memory recall, and the related task of thinking about plausible personal future events, have revealed a consistent network of associated brain regions. Surprisingly little, however, is understood about the contributions individual brain areas make to the overall recollective experience. To examine this, we used a novel fMRI paradigm in which subjects had to imagine fictitious experiences. In contrast to future thinking, this results in experiences that are not explicitly temporal in nature or as reliant on self-processing. By using previously imagined fictitious experiences as a comparison for episodic memories, we identified the neural basis of a key process engaged in common, namely scene construction, involving the generation, maintenance and visualization of complex spatial contexts. This was associated with activations in a distributed network, including hippocampus, parahippocampal gyrus, and retrosplenial cortex. Importantly, we disambiguated these common effects from episodic memory-specific responses in anterior medial prefrontal cortex, posterior cingulate cortex and precuneus. These latter regions may support self-schema and familiarity processes, and contribute to the brain's ability to distinguish real from imaginary memories. We conclude that scene construction constitutes a common process underlying episodic memory and imagination of fictitious experiences, and suggest it may partially account for the similar brain networks implicated in navigation, episodic future thinking, and the default mode. We suggest that additional brain regions are co-opted into this core network in a task-specific manner to support functions such as episodic memory that may have additional requirements.},
	language = {en},
	number = {52},
	urldate = {2024-10-15},
	journal = {The Journal of Neuroscience},
	author = {Hassabis, Demis and Kumaran, Dharshan and Maguire, Eleanor A.},
	month = dec,
	year = {2007},
	pages = {14365--14374},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/NVPPUFG3/Hassabis et al. - 2007 - Using Imagination to Understand the Neural Basis of Episodic Memory.pdf:application/pdf},
}

@article{carey_theories_2015,
	title = {Theories of development: {In} dialog with {Jean} {Piaget}},
	volume = {38},
	issn = {02732297},
	shorttitle = {Theories of development},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0273229715000313},
	doi = {10.1016/j.dr.2015.07.003},
	abstract = {Piaget’s body of work had two major theoretical thrusts: constructivism and stage theory. Both constructivism and stage theories articulate modern work on conceptual development, albeit transformed by developments in cognitive science and cognitive neuroscience. A case study of conceptual change in childhood within a framework theory of intuitive biology illustrates these points.},
	language = {en},
	urldate = {2024-11-05},
	journal = {Developmental Review},
	author = {Carey, Susan and Zaitchik, Deborah and Bascandziev, Igor},
	month = dec,
	year = {2015},
	pages = {36--54},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2LG57I72/Carey et al. - 2015 - Theories of development In dialog with Jean Piaget.pdf:application/pdf},
}

@book{bartlett_remembering_1995,
	edition = {2},
	title = {Remembering: {A} {Study} in {Experimental} and {Social} {Psychology}},
	copyright = {https://www.cambridge.org/core/terms},
	isbn = {978-0-521-48356-8 978-0-521-48278-3 978-0-511-75918-5},
	shorttitle = {Remembering},
	url = {https://www.cambridge.org/core/product/identifier/9780511759185/type/book},
	abstract = {In 1932, Cambridge University Press published Remembering, by psychologist, Frederic Bartlett. The landmark book described fascinating studies of memory and presented the theory of schema which informs much of cognitive science and psychology today. In Bartlett's most famous experiment, he had subjects read a Native American story about ghosts and had them retell the tale later. Because their background was so different from the cultural context of the story, the subjects changed details in the story that they could not understand. Based on observations like these, Bartlett developed his claim that memory is a process of reconstruction, and that this construction is in important ways a social act. His concerns about the social psychology of memory and the cultural context of remembering were long neglected but are finding an interested and responsive audience today. Now reissued in paperback, Remembering has a new Introduction by Walter Kintsch of the University of Colorado, Boulder.},
	urldate = {2025-02-06},
	publisher = {Cambridge University Press},
	author = {Bartlett, Frederic C. and Kintsch, Walter},
	month = jun,
	year = {1995},
	doi = {10.1017/CBO9780511759185},
	file = {Remembering_A_Study_in_Experimental_and_Social_Psychology_Frederic:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/IAEPKU8W/Remembering_A_Study_in_Experimental_and_Social_Psychology_Frederic.pdf:application/pdf},
}

@article{pineda_entropic_2021,
	title = {An entropic associative memory},
	volume = {11},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-86270-7},
	doi = {10.1038/s41598-021-86270-7},
	abstract = {Abstract
            Natural memories are associative, declarative and distributed, and memory retrieval is a constructive operation. In addition, cues of objects that are not contained in the memory are rejected directly. Symbolic computing memories resemble natural memories in their declarative character, and information can be stored and recovered explicitly; however, they are reproductive rather than constructive, and lack the associative and distributed properties. Sub-symbolic memories developed within the connectionist or artificial neural networks paradigm are associative and distributed, but lack the declarative property, the capability of rejecting objects that are not included in the memory, and memory retrieval is also reproductive. In this paper we present a memory model that sustains the five properties of natural memories. We use Relational-Indeterminate Computing to model associative memory registers that hold distributed representations of individual objects. This mode of computing has an intrinsic computing entropy which measures the indeterminacy of representations. This parameter determines the operational characteristics of the memory. Associative registers are embedded in an architecture that maps concrete images expressed in modality specific buffers into abstract representations and vice versa. The framework has been used to model a visual memory holding the representations of hand-written digits. The system has been tested with a set of memory recognition and retrieval experiments with complete and severely occluded images. The results show that there is a range of entropy values, not too low and not too high, in which associative memory registers have a satisfactory performance. The experiments were implemented in a simulation using a standard computer with a GPU, but a parallel architecture may be built where the memory operations would take a very reduced number of computing steps.},
	language = {en},
	number = {1},
	urldate = {2025-02-20},
	journal = {Scientific Reports},
	author = {Pineda, Luis A. and Fuentes, Gibrán and Morales, Rafael},
	month = mar,
	year = {2021},
	pages = {6948},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/2WT6NJYR/Pineda et al. - 2021 - An entropic associative memory.pdf:application/pdf},
}

@article{pineda_weighted_2022,
	title = {Weighted entropic associative memory and phonetic learning},
	volume = {12},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-022-20798-0},
	doi = {10.1038/s41598-022-20798-0},
	abstract = {Abstract
            The Entropic Associative Memory (EAM) holds declarative but distributed representations of remembered objects. These are characterized as functions from features to discrete values in an abstract amodal space. Memory objects are registered or remembered through a declarative operation; memory recognition is defined as a logical test and cues of objects not contained in the memory are rejected directly without search; and memory retrieval is a constructive operation. In its original formulation, the content of basic memory units or cells was either on or off, hence all stored objects had the same weight or strength. In the present weighted version (W-EAM) we introduce a basic learning mechanism to the effect that the values of the cells used in the representation of an object are reinforced by the memory register operation. As memory cells are shared by different representations, the corresponding associations are reinforced too. The memory system supports a second form of learning: the distributed representation generalizes and renders a large set of potential or latent units that can used for recognizing novel inputs, which can in turn be used for improving the performance of both the deep neural networks used for modelling perception and action, and of the memory operations. This process can be performed recurrently in open-ended fashion and can be used in long term learning. An experiment in the phonetic domain using the Mexican Spanish DIMEx100 Corpus was carried out. This corpus was collected in a controlled noise-free environment, and was transcribed manually by human trained phoneticians, but consists of a relatively small number of utterances. DIMEx100 was used to produced the initial state of the perceptual and motor modules, and for testing the performance of the memory system at such state. Then the incremental learning cycle was modelled using the Spanish CIEMPIESS Corpus, consisting of a very large number of noisy untagged speech utterances collected from radio and TV. The results support the viability of the Weighted Entropic Associative Memory for modelling cognitive processes, such as phonetic representation and learning, for the construction of applications, such as speech recognition and synthesis, and as a computational model of natural memory.},
	language = {en},
	number = {1},
	urldate = {2025-02-27},
	journal = {Scientific Reports},
	author = {Pineda, Luis A. and Morales, Rafael},
	month = oct,
	year = {2022},
	pages = {16703},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/EWP29LBX/Pineda y Morales - 2022 - Weighted entropic associative memory and phonetic learning.pdf:application/pdf},
}

@article{pineda_imagery_2023,
	title = {Imagery in the entropic associative memory},
	volume = {13},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-36761-6},
	doi = {10.1038/s41598-023-36761-6},
	abstract = {Abstract
            The Entropic Associative Memory is a novel declarative and distributed computational model of associative memory. The model is general, conceptually simple, and offers an alternative to models developed within the artificial neural networks paradigm. The memory uses a standard table as its medium, where the information is stored in an indeterminate form, and the entropy plays a functional and operation role. The memory register operation abstracts the input cue with the current memory content and is productive; memory recognition is performed through a logical test; and memory retrieval is constructive. The three operations can be performed in parallel using very few computing resources. In our previous work we explored the auto-associative properties of the memory and performed experiments to store, recognize and retrieve manuscript digits and letters with complete and incomplete cues, and also to recognize and learn phones, with satisfactory results. In such experiments a designated memory register was used to store all the objects of the same class, whereas in the present study we remove such restriction and use a single memory register to store all the objects in the domain. In this novel setting we explore the production of emerging objects and relations, such that cues are used not only to retrieve remembered objects, but also related and imaged objects, and to produce association chains. The present model supports the view that memory and classification are independent functions both conceptually and architecturally. The memory system can store images of the different modalities of perception and action, possibly multimodal, and offers a novel perspective on the imagery debate and computational models of declarative memory.},
	language = {en},
	number = {1},
	urldate = {2025-02-27},
	journal = {Scientific Reports},
	author = {Pineda, Luis A. and Morales, Rafael},
	month = jun,
	year = {2023},
	pages = {9553},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/5TWBCBD9/Pineda y Morales - 2023 - Imagery in the entropic associative memory.pdf:application/pdf},
}

@incollection{schendan_semantic_2012,
	title = {Semantic {Memory}},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	isbn = {978-0-08-096180-4},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780123750006003153},
	language = {en},
	urldate = {2025-04-28},
	booktitle = {Encyclopedia of {Human} {Behavior}},
	publisher = {Elsevier},
	author = {Schendan, H.E.},
	year = {2012},
	doi = {10.1016/B978-0-12-375000-6.00315-3},
	pages = {350--358},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/JBIRHJTX/Schendan - 2012 - Semantic Memory.pdf:application/pdf},
}

@article{duff_semantic_2020,
	title = {Semantic {Memory} and the {Hippocampus}: {Revisiting}, {Reaffirming}, and {Extending} the {Reach} of {Their} {Critical} {Relationship}},
	volume = {13},
	issn = {1662-5161},
	shorttitle = {Semantic {Memory} and the {Hippocampus}},
	url = {https://www.frontiersin.org/article/10.3389/fnhum.2019.00471/full},
	doi = {10.3389/fnhum.2019.00471},
	abstract = {Since Tulving proposed a distinction in memory between semantic and episodic memory, considerable effort has been directed towards understanding their similar and unique features. Of particular interest has been the extent to which semantic and episodic memory have a shared dependence on the hippocampus. In contrast to the deﬁnitive evidence for the link between hippocampus and episodic memory, the role of the hippocampus in semantic memory has been a topic of considerable debate. This debate stems, in part, from highly variable reports of new semantic memory learning in amnesia ranging from profound impairment to full preservation, and various degrees of deﬁcit and ability in between. More recently, a number of signiﬁcant advances in experimental methods have occurred, alongside new provocative data on the role of the hippocampus in semantic memory, making this an ideal moment to revisit this debate, to re-evaluate data, methods, and theories, and to synthesize new ﬁndings. In line with these advances, this review has two primary goals. First, we provide a historical lens with which to reevaluate and contextualize the literature on semantic memory and the hippocampus. The second goal of this review is to provide a synthesis of new ﬁndings on the role of the hippocampus and semantic memory. With the perspective of time and this critical review, we arrive at the interpretation that the hippocampus does indeed make necessary contributions to semantic memory. We argue that semantic memory, like episodic memory, is a highly ﬂexible, (re)constructive, relational and multimodal system, and that there is value in developing methods and materials that fully capture this depth and richness to facilitate comparisons to episodic memory. Such efforts will be critical in addressing questions regarding the cognitive and neural (inter)dependencies among forms of memory, and the role that these forms of memory play in support of cognition more broadly. Such efforts also promise to advance our understanding of how words, concepts, and meaning, as well as episodes and events, are instantiated and maintained in memory and will yield new insights into our two most quintessentially human abilities: memory and language.},
	language = {en},
	urldate = {2025-05-13},
	journal = {Frontiers in Human Neuroscience},
	author = {Duff, Melissa C. and Covington, Natalie V. and Hilverman, Caitlin and Cohen, Neal J.},
	month = jan,
	year = {2020},
	pages = {471},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/P5GFD83Y/Duff et al. - 2020 - Semantic Memory and the Hippocampus Revisiting, Reaffirming, and Extending the Reach of Their Criti.pdf:application/pdf},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	language = {en},
	urldate = {2025-08-22},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 15 pages, 5 figures},
	annote = {Comment: 15 pages, 5 figures},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RDM4KCQ5/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf},
}

@book{jurafsky_speech_2025,
	edition = {3rd},
	title = {Speech and {Language} {Processing}: {An} {Introduction} to {Natural} {Language} {Processing},    		  {Computational} {Linguistics}, and {Speech} {Recognition}, 		   with {Language} {Models}},
	url = {https://web.stanford.edu/~jurafsky/slp3/},
	author = {Jurafsky, Daniel and Martin, James H.},
	year = {2025},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SN83NG3V/ed3book.pdf:application/pdf},
}

@article{corballis_language_2019,
	title = {Language, {Memory}, and {Mental} {Time} {Travel}: {An} {Evolutionary} {Perspective}},
	volume = {13},
	issn = {1662-5161},
	shorttitle = {Language, {Memory}, and {Mental} {Time} {Travel}},
	url = {https://www.frontiersin.org/article/10.3389/fnhum.2019.00217/full},
	doi = {10.3389/fnhum.2019.00217},
	abstract = {Language could not exist without memory, in all its forms: working memory for sequential production and understanding, implicit memory for grammatical rules, semantic memory for knowledge, and episodic memory for communicating personal experience. Episodic memory is part of a more general capacity for mental travel both forward and backward in time, and extending even into fantasy and stories. I argue that the generativity of mental time travel underlies the generativity of language itself, and could be the basis of what Chomsky calls I-language, or universal grammar (UG), a capacity for recursive thought independent of communicative language itself. Whereas Chomsky proposed that I-language evolved in a single step well after the emergence of Homo sapiens, I suggest that generative imagination, extended in space and time, has a long evolutionary history, and that it was the capacity to share internal thoughts, rather than the nature of the thoughts themselves, that more clearly distinguishes humans from other species.},
	language = {en},
	urldate = {2025-08-25},
	journal = {Frontiers in Human Neuroscience},
	author = {Corballis, Michael C.},
	month = jul,
	year = {2019},
	pages = {217},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/X5SIUNZY/Corballis - 2019 - Language, Memory, and Mental Time Travel An Evolutionary Perspective.pdf:application/pdf},
}

@article{mascelloni_distinct_2019,
	title = {Distinct {Neural} {Processes} for {Memorizing} {Form} and {Meaning} {Within} {Sentences}},
	volume = {13},
	issn = {1662-5161},
	url = {https://www.frontiersin.org/article/10.3389/fnhum.2019.00412/full},
	doi = {10.3389/fnhum.2019.00412},
	abstract = {In order to memorize sentences we use both processes of language comprehension during encoding and processes of language production during maintenance. While the former processes are easily testable via controlled presentation of the input, the latter are more difﬁcult to assess directly as language production is typically initiated and controlled internally. In the present event-related potential (ERP) study we track subvocal rehearsal of sentences, with the goal of studying the concomitant planning processes with the help of a silent cued-production task. Native German participants read different types of sentences word-by-word, then were prompted by a visual cue to silently repeat each individual word, in a rehearsal phase. In order to assess both local and global effects of sentence planning, we presented correct sentences, syntactically or semantically violated sentences, or random word order sequences. Semantic violations during reading elicited an N400 effect at the noun violating the selectional restrictions of the preceding verb. Syntactic violations, induced by a gender incongruency between determiner and noun, led to a P600 effect at the same position. Different ERP patterns occurred during the silent production phase. Here, semantically violated sentences elicited an early fronto-central negativity at the verb, while syntactically violated sentences elicited a late right-frontal positivity at the determiner. Random word order was accompanied by long-lasting slow waves during the production phase. The ﬁndings are consistent with models of hierarchical sentence planning and further indicate that the ongoing working memory processes are qualitatively distinct from comprehension mechanisms and neurophysiologically speciﬁc for syntactic and lexical-semantic level planning. In conclusion, active working memory maintenance of sentences is likely to comprise speciﬁc stages of sentence production that are indicated by ERP correlates of syntactic and semantic planning at the phrasal and clausal level respectively.},
	language = {en},
	urldate = {2025-08-25},
	journal = {Frontiers in Human Neuroscience},
	author = {Mascelloni, Matteo and Zamparelli, Roberto and Vespignani, Francesco and Gruber, Thomas and Mueller, Jutta L.},
	month = dec,
	year = {2019},
	pages = {412},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/YWI967CU/Mascelloni et al. - 2019 - Distinct Neural Processes for Memorizing Form and Meaning Within Sentences.pdf:application/pdf},
}

@article{warren_cross-situational_nodate,
	title = {Cross-{Situational} {Statistical} {Learning} of {New} {Words} {Despite} {Bilateral} {Hippocampal} {Damage} and {Severe} {Amnesia}},
	abstract = {Word learning requires learners to bind together arbitrarily-related phonological, visual, and conceptual information. Prior work suggests that this binding can be robustly achieved via incidental cross-situational statistical exposure to words and referents. When cross-situational statistical learning (CSSL) is tested in the laboratory, there is no information on any given trial to identify the referent of a novel word. However, by tracking which objects co-occur with each word across trials, learners may acquire mappings through statistical association. While CSSL behavior is well-characterized, its brain correlates are not. The arbitrary nature of CSSL mappings suggests hippocampal involvement, but the incremental, statistical nature of the learning raises the possibility of neocortical or procedural learning systems. Prior studies have shown that neurological patients with hippocampal pathology have word-learning impairments, but this has not been tested in a statistical learning paradigm. Here, we used a neuropsychological approach to test whether patients with bilateral hippocampal pathology (N = 3) could learn new words in a CSSL paradigm. In the task, patients and healthy comparison participants completed a CSSL word-learning task in which they acquired eight word/object mappings. During each trial of the CSSL task, participants saw two objects on a computer display, heard one novel word, and selected the most likely referent. Across trials, words were 100\% likely to co-occur with their referent, but only 14.3\% likely with non-referents. Two of three amnesic patients learned the associations between objects and word forms, although performance was impaired relative to healthy comparison participants. Our ﬁndings show that the hippocampus is not strictly necessary for CSSL for words, although it may facilitate such learning. This is consistent with a hybrid account of CSSL supported by implicit and explicit memory systems, and may have translational applications for remediation of (word-) learning deﬁcits in neurological populations with hippocampal pathology.},
	language = {en},
	author = {Warren, David E},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/SMM4S2TB/Warren - Cross-Situational Statistical Learning of New Words Despite Bilateral Hippocampal Damage and Severe.pdf:application/pdf},
}

@book{gray_entropy_1990,
	address = {New York, NY},
	title = {Entropy and {Information} {Theory}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4757-3984-8 978-1-4757-3982-4},
	url = {http://link.springer.com/10.1007/978-1-4757-3982-4},
	language = {en},
	urldate = {2025-08-26},
	publisher = {Springer New York},
	author = {Gray, Robert M.},
	year = {1990},
	doi = {10.1007/978-1-4757-3982-4},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/EUTW3VNX/Gray - 1990 - Entropy and Information Theory.pdf:application/pdf},
}

@article{shannon_mathematical_nodate,
	title = {A {Mathematical} {Theory} of {Communication}},
	language = {en},
	author = {Shannon, C E},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/44W2CCWB/Shannon - A Mathematical Theory of Communication.pdf:application/pdf},
}

@book{tunstall_natural_2022,
	address = {Sebastopol, California},
	edition = {Revised edition},
	title = {Natural language processing with transformers: building language applications with {Hugging} {Face}},
	isbn = {978-1-0981-3678-9},
	shorttitle = {Natural language processing with transformers},
	abstract = {"Since their introduction in 2017, transformers have quickly become the dominant architecture for achieving state-of-the-art results on a variety of natural language processing tasks. If you're a data scientist or coder, this practical book -now revised in full color- shows you how to train and scale these large models using Hugging Face Transformers, a Python-based deep learning library.Transformers have been used to write realistic news stories, improve Google Search queries, and even create chatbots that tell corny jokes. In this guide, authors Lewis Tunstall, Leandro von Werra, and Thomas Wolf, among the creators of Hugging Face Transformers, use a hands-on approach to teach you how transformers work and how to integrate them in your applications. You'll quickly learn a variety of tasks they can help you solve. Build, debug, and optimize transformer models for core NLP tasks, such as text classification, named entity recognition, and question answering; Learn how transformers can be used for cross-lingual transfer learning; Apply transformers in real-world scenarios where labeled data is scarce; Make transformer models efficient for deployment using techniques such as distillation, pruning, and quantization; Train transformers from scratch and learn how to scale to multiple GPUs and distributed environments." -- provided by publisher},
	language = {eng},
	publisher = {O'Reilly Media, Inc.},
	author = {Tunstall, Lewis and Werra, Leandro van and Wolf, Thomas},
	year = {2022},
	note = {OCLC: 1395782862},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/RTWUVDTA/Tunstall et al. - 2022 - Natural language processing with transformers building language applications with Hugging Face.pdf:application/pdf},
}

@misc{pineda_entropy_2020,
	title = {Entropy, {Computing} and {Rationality}},
	url = {http://arxiv.org/abs/2009.10224},
	doi = {10.48550/arXiv.2009.10224},
	abstract = {Making decisions freely presupposes that there is some indeterminacy in the environment and in the decision making engine. The former is reﬂected on the behavioral changes due to communicating: few changes indicate rigid environments; productive changes manifest a moderate indeterminacy, but a large communicating eﬀort with few productive changes characterize a chaotic environment. Hence, communicating, eﬀective decision making and productive behavioral changes are related. The entropy measures the indeterminacy of the environment, and there is an entropy range in which communicating supports eﬀective decision making. This conjecture is referred to here as the The Potential Productivity of Decisions.},
	language = {en},
	urldate = {2025-11-04},
	publisher = {arXiv},
	author = {Pineda, Luis A.},
	month = sep,
	year = {2020},
	note = {arXiv:2009.10224 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 43 pages, 4 figures, 44 references},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/CQY3PES3/Pineda - 2020 - Entropy, Computing and Rationality.pdf:application/pdf},
}

@book{foster_generative_2023,
	address = {Beijing Boston Farnham Sebastopol Tokyo},
	edition = {Second edition},
	title = {Generative deep learning: teaching machines to paint, write, compose, and play},
	isbn = {978-1-0981-3418-1},
	shorttitle = {Generative deep learning},
	language = {en},
	publisher = {O'Reilly},
	author = {Foster, David and Friston, Karl J.},
	year = {2023},
	file = {PDF:/Users/roicort/Library/Mobile Documents/com~apple~CloudDocs/Zotero/storage/AWXTT3QE/Foster y Friston - 2023 - Generative deep learning teaching machines to paint, write, compose, and play.pdf:application/pdf},
}
